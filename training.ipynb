{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339d54b6",
   "metadata": {},
   "source": [
    "# Training Tacotron2\n",
    "Warm start shouldn't be enabled if training from one of your own checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9638948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdir = \"./lyre/tacotron_checkpoints/experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbed6eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.15.0 at http://0.0.0.0:6006/ (Press CTRL+C to quit)\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "FP16 Run: True\n",
      "Dynamic Loss Scaling: True\n",
      "Distributed Run: False\n",
      "cuDNN Enabled: True\n",
      "cuDNN Benchmark: False\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "Warm starting model from checkpoint './lyre/tacotron2_statedict.pt'\n",
      "Epoch: 0\n",
      "/mnt/c/Users/david/source/obama-tts/tacotron2/utils.py:14: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "^C\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    }
   ],
   "source": [
    "!python tacotron2/train.py \\\n",
    "--output_directory $outdir \\\n",
    "--log_directory logs \\\n",
    "--checkpoint_path ./lyre/tacotron2_statedict.pt \\\n",
    "--warm_start \\\n",
    "& \\\n",
    "tensorboard --logdir $outdir/logs --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d5cf1",
   "metadata": {},
   "source": [
    "# Training WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc42a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
      "output directory ./lyre/waveglow_checkpoints\n",
      "Epoch: 0\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:56: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:57: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554789074/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(data).float(), sampling_rate\n",
      "0:\t0.000120620\n",
      "Saving model and optimizer state at iteration 0 to ./lyre/waveglow_checkpoints/waveglow_0\n",
      "1:\t-0.018064046\n",
      "2:\t-0.056628060\n",
      "3:\t-0.116746739\n",
      "Epoch: 1\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:56: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:57: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554789074/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(data).float(), sampling_rate\n",
      "4:\t-0.225309461\n",
      "5:\t-0.397439480\n",
      "6:\t-0.554910481\n",
      "7:\t-0.815065265\n",
      "Epoch: 2\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:56: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:57: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554789074/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(data).float(), sampling_rate\n",
      "8:\t-1.217895985\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "9:\t-0.161647871\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "10:\t-1.368163347\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "11:\t-0.547898471\n",
      "Epoch: 3\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:56: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate, data = read(full_path)\n",
      "/mnt/c/Users/david/source/obama-tts/waveglow/mel2samp.py:57: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554789074/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(data).float(), sampling_rate\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "12:\t-0.648699343\n",
      "13:\t-0.417315841\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "14:\t-1.707578659\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"waveglow/train.py\", line 188, in <module>\n",
      "    train(num_gpus, args.rank, args.group_name, **train_config)\n",
      "  File \"waveglow/train.py\", line 135, in train\n",
      "    scaled_loss.backward()\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/handle.py\", line 123, in scale_loss\n",
      "    optimizer._post_amp_backward(loss_scaler)\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/_process_optimizer.py\", line 241, in post_backward_no_master_weights\n",
      "    post_backward_models_are_masters(scaler, params, stashed_grads)\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/_process_optimizer.py\", line 127, in post_backward_models_are_masters\n",
      "    scale_override=(grads_have_scale, stashed_have_scale, out_scale))\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/scaler.py\", line 184, in unscale_with_stashed\n",
      "    out_scale/stashed_have_scale)\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/scaler.py\", line 148, in unscale_with_stashed_python\n",
      "    self.dynamic)\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/scaler.py\", line 30, in axpby_check_overflow_python\n",
      "    master_grad.data = a*converted_model_grad.data + b*stashed_grad.data\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/wrap.py\", line 50, in wrapper\n",
      "    types = utils.collect_fp_tensor_types(args, kwargs)\n",
      "  File \"/home/dcalvo/miniconda3/envs/tacotron2/lib/python3.6/site-packages/apex/amp/utils.py\", line 46, in collect_fp_tensor_types\n",
      "    for x in all_args:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python waveglow/train.py -c waveglow/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94892d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
